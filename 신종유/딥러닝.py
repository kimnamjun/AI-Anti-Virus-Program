# -*- coding: utf-8 -*-
"""딥러닝.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mMKoGArmDdUPEVIKo4hw2cl4V8tP3xvf
"""

from google.colab import drive
drive.mount('/content/drive')

import re
import json
import numpy as np
import pandas as pd

path = '/content/drive/MyDrive/dataset/JY_Dataset'
train_df = pd.read_csv(path + '/train_main_features+length_JY.csv')
test_df = pd.read_csv(path + '/test_main_features+length_JY.csv')

# 셔플 후 샘플링
train_df = train_df.sample(frac=1).reset_index(drop=True)
train_df = train_df.head(900)
#train_df

# 셔플 후 샘플링
test_df = test_df.sample(frac=1).reset_index(drop=True)
test_df = test_df.head(300)
#test_df

"""### **리스트화+차원화**"""

# 리스트화
df = pd.DataFrame(train_df)
df['imports'] = df['imports'].map(lambda x: eval(x))
train_tmp = df.values.tolist()
#train_tmp

# imports 칼럼 279 차원으로 변환
MAX_LENGTH = 279
for idx, (sha, label, functions, length) in enumerate(train_tmp):
    functions += [''] * (MAX_LENGTH - length)
    train_tmp[idx][2] = functions

df = pd.DataFrame(test_df)
df['imports'] = df['imports'].map(lambda x: eval(x))
test_tmp = df.values.tolist()
#test_tmp

MAX_LENGTH = 279
for idx, (sha, label, functions, length) in enumerate(test_tmp):
    functions += [''] * (MAX_LENGTH - length)
    test_tmp[idx][2] = functions

"""### **train/test, 독립/종속 그룹화**"""

from tqdm.notebook import tqdm

def data_processing():

    train_x = []
    train_y = []
    
    for i in tqdm(range(len(train_tmp))):
        noun_adjective = train_tmp[i][2]
        train_x.append(noun_adjective)
        train_y.append(int(train_tmp[i][1]))

    test_x = []
    test_y = []

    for i in tqdm(range(len(test_tmp))):
        noun_adjective = test_tmp[i][2]
        test_x.append(noun_adjective)
        test_y.append(int(test_tmp[i][1]))

    return (train_x, train_y) , (test_x, test_y)

#print('...그룹 분리...')
(X_train, y_train),(X_test,y_test) = data_processing()

# 변수 지우기
del train_tmp
del test_tmp

"""### **벡터화**"""

from gensim.models import Word2Vec

word2vector = Word2Vec(X_train, size=279, window=10, min_count=1, sg=1)

# 변수 지우기
word2vector.init_sims(replace=True)

word_vector_keys=word2vector.wv.vocab.keys()
#word_vector_keys

#X_train의 문자열을 Word2Vector 로 변환해서 저장할 배열
X_train_vector=[]
#len(X_train) : X_train의 줄의 수만큼 반복
for index in tqdm(range(len(X_train))):
    input=[]
    #X_train[index]: X_train (API 함수 저장된 리스트)
    #                     index번째줄 

    #단어를 word에 대입
    for word in X_train[index]:
        #단어 word가 word_vector_keys(Word2Vector에서 벡터로 변환한 함수 리스트) 에 포함되 있으면
        if word in word_vector_keys:
            #word2vector[word] : 단어의 word2vector 값을 input에 추가
            input.append(word2vector[word].tolist())
        else: 
            #word2vector[""] : ""의 word2vector 값을 input에 추가
            input.append(word2vector[""].tolist()) 

    X_train_vector.append(input)

#X_train_vector 를 numpy 배열로 변환
X_train_vector = np.array(X_train_vector, dtype="float32")

del X_train

#X_test 의 문자열을 Word2Vector 로 변환해서 저장할 배열
X_test_vector=[]
#len(X_test) : X_test의 줄의 수만큼 반복
for index in tqdm(range(len(X_test))):
    input=[]
    #X_test[index]: X_test (API 함수가 저장된 리스트)
    #                     index번째줄 

    #단어를 word에 대입
    for word in X_test[index]:
        #단어 word가 word_vector_keys(Word2Vector에서 벡터로 변환한 단어 리스트) 에 포함되 있으면
        if word in word_vector_keys:
            #word2vector[word] : 단어의 word2vector 값을 input에 추가
            input.append(word2vector[word].tolist())
        else: 
            #word2vector[""] : ""의 word2vector 값을 input에 추가
            input.append(word2vector[""].tolist()) 

    X_test_vector.append(input)

#메모리에서 X_test 삭제
del X_test

from tensorflow.keras import Sequential
#예측을 실행할 모델 객체 생성
model = Sequential()

from tensorflow.keras.layers import LSTM

#LSTM 실행 객체
model.add(LSTM(units=128, input_shape=(279,279)))

from tensorflow.keras.layers import Dense
#결과의 확률을 계산할 레이어 0(부정),1(긍정) 2개의 값중 하나가 리턴됨
model.add(Dense(1,activation="sigmoid"))

#model의 정보를 조회
model.summary()

from tensorflow.keras.optimizers import Adam
#모델을 실행할 준비를 함
model.compile(loss='binary_crossentropy',
            optimizer=Adam(lr=1e-3),
            metrics=['acc'])

#y_train을 float32 타입으로 변환
y_train=np.array(y_train, dtype="float32")

#y_train

#학습 시작
model.fit(X_train_vector, y_train,
          batch_size=1000,
          epochs=10)

#y_test

#y_test를 float32타입으로 변환
y_test=np.array(y_test, dtype="float32")

#X_test_vector

#X_test_vector를 float32타입으로 변환
X_test_vector=np.array(X_test_vector, dtype="float32")

#X_test_vector

#X_test_vector(테스트 데이터) 의 cost와 정확도 조회
model.evaluate(X_test_vector, y_test)

