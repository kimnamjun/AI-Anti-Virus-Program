# -*- coding: utf-8 -*-
"""1201_7조_Attention.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1N_CXLpziMpMU4kjw7heO5Fbpw0Tjo_2I
"""

from google.colab import drive
drive.mount('/content/drive')

import re
import json
import pickle
import numpy as np
import pandas as pd
from google.colab import drive

import tensorflow as tf

PATH = '/content/drive/MyDrive/dataset/'
MAX_LENGTH = 389
VECTOR_SIZE = 10

"""## 데이터 전처리

## 데이터 읽어 오기
"""

with open(PATH + 'JY_Dataset/LSTM/train_df.pickle', 'rb') as file:
    train_df = pickle.load(file)

with open(PATH + 'JY_Dataset/LSTM/test_df.pickle', 'rb') as file:
    test_df = pickle.load(file)

train_df

test_df

"""# 데이터 셋 생성"""

# 전체 데이터 수
BUFFER_SIZE = len(train_df)
# 배치 사이즈
BATCH_SIZE = 512

# train_df["imports"] 컬럼의 데이터를 문자열로 (리스트가 저장되 있음) 변환해서 리턴
x_train = train_df["imports"].apply(lambda row: " ".join(row))

# Series타입의 x_train을 list로 변환해서 리턴
x_train = x_train.to_list()

x_train

# 악성 코드 여부를 list로 변환해서 y_train에 대입
y_train = train_df["label"].to_list()

y_train

# y_train을 numpy 배열로 변환 후 float32타입으로 변환
y_train = np.array( y_train) .astype("float32")

y_train

# tf.data.Dataset.from_tensor_slices((x_train, y_train) ): x_train, y_train을 BATCH_SIZE 씩 리턴할 객체 생성
train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train) ).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)



# test_df["imports"] 컬럼의 데이터를 문자열로 (리스트가 저장되 있음) 변환해서 리턴
x_test = test_df["imports"].apply(lambda row: " ".join(row))

# Series타입의 x_test 을 list로 변환해서 리턴
x_test = x_test.to_list()

x_test

# 악성 코드 여부를 list로 변환해서 y_test 에 대입
y_test = test_df["label"].to_list()

y_test

# y_test 을 numpy 배열로 변환 후 float32타입으로 변환
y_test = np.array( y_test) .astype("float32")

y_test

# tf.data.Dataset.from_tensor_slices((x_test, y_test) ): x_test, y_test 을 BATCH_SIZE 씩 리턴할 객체 생성
test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test) ).batch(BATCH_SIZE)

"""# 단어를 숫자로 변환하는 레이어 구현"""

from tensorflow.keras.layers.experimental.preprocessing import TextVectorization

# TextVectorization : 문자열의 단어를 숫자로 변환하는 레이어
vectorize_layer = TextVectorization(
                        output_mode='int', # 정수타입 숫자로 변환됨
                        output_sequence_length=400) # 변환할 단어의 개수

# train_ds 에서 데이터를 리턴받아서 단어의 개수 등을 파악
# train_ds.map(lambda text, label: text) : train_ds 에서 label과 text 를 매개변수로 받아서 text를 리턴

# vectorize_layer.adapt : train_ds에서 리턴받은 text의 단어개수 빈도수등을 파악해서 vectorize_layer에 저장 
vectorize_layer.adapt(train_ds.map(lambda text, label:   text))

# vectorie_layer가 숫자로 변환 할 단어 조회
vectorize_layer.get_vocabulary()

# vectorize_layer 가 숫자로 변환할 단어의 개수
len(vectorize_layer.get_vocabulary())



"""# TextVectorization 테스트
- 대문자는 소문자로 자동 변환
- 특수문자는 자동 삭제됨
"""

# train_ds.take(1) : train_ds에서 BATCH_SIZE 만큼 데이터 리턴
# 문자열을 example, 악성코드 여부는 label에 저장
for example, label in train_ds.take(1):
  print('texts: ', example.numpy())
  print()
  print('labels: ', label.numpy())

# vectorize_layer(example) : example에 저장된 단어를 숫자로 변환
encoded_example = vectorize_layer(example).numpy()
encoded_example

# vectorize_layer.get_vocabulary() : vectorize_layer가 숫자로 변환할 단어 조회
vocab = np.array(vectorize_layer.get_vocabulary())

vocab

vocab.shape

for n in range(3):
  # 문자열 단어를 숫자로 변환한 결과 조회  
  print("Original: ", encoded_example[n][0:10])
  # 어떤 단어가 변환 되었는지 조회
  print("Round-trip: ", " ".join ( vocab[encoded_example[n][0:10]]) )
  print()

from tensorflow.keras.layers import Dense, Bidirectional, LSTM, Concatenate, Dropout,Embedding
from tensorflow.keras import Input, Model,Sequential
from tensorflow.keras import optimizers
import tensorflow as tf
from tensorflow.keras.metrics import BinaryCrossentropy

"""
## 양뱡향 LSTM 과 Attention"""

class BahdanauAttention(Model):
  def __init__(self, units):
    super(BahdanauAttention, self).__init__()
    self.W1 = Dense(units)
    self.W2 = Dense(units)
    self.V = Dense(1)

  def call(self, values, query): # 단, key와 value는 같음
    # query shape == (batch_size, hidden size)
    # hidden_with_time_axis shape == (batch_size, 1, hidden size)
    # score 계산을 위해 뒤에서 할 덧셈을 위해서 차원을 변경해줍니다.
    hidden_with_time_axis = tf.expand_dims(query, 1)

    # score shape == (batch_size, max_length, 1)
    # we get 1 at the last axis because we are applying score to self.V
    # the shape of the tensor before applying self.V is (batch_size, max_length, units)
    score = self.V(tf.nn.tanh(
        self.W1(values) + self.W2(hidden_with_time_axis)))

    # attention_weights shape == (batch_size, max_length, 1)
    attention_weights = tf.nn.softmax(score, axis=1)

    # context_vector shape after sum == (batch_size, hidden_size)
    context_vector = attention_weights * values
    context_vector = tf.reduce_sum(context_vector, axis=1)

    return context_vector, attention_weights

sequence_input = Input(shape=(1,), dtype='string')
vector_input = vectorize_layer(sequence_input)
embedded_sequences = Embedding(len(vectorize_layer.get_vocabulary()) , 128, mask_zero = True)(vector_input)

lstm = Bidirectional(LSTM(64, dropout=0.5, return_sequences = True))(embedded_sequences)

lstm, forward_h, forward_c, backward_h, backward_c = Bidirectional (LSTM(64, dropout=0.5, return_sequences=True, return_state=True))(lstm)

print(lstm.shape, forward_h.shape, forward_c.shape, backward_h.shape, backward_c.shape)

state_h = Concatenate()([forward_h, backward_h]) # 은닉 상태
state_c = Concatenate()([forward_c, backward_c]) # 셀 상태

attention = BahdanauAttention(64) # 가중치 크기 정의
context_vector, attention_weights = attention(lstm, state_h)

dense1 = Dense(20, activation="relu")(context_vector)
dropout = Dropout(0.5)(dense1)
output = Dense(1, activation="sigmoid")(dropout)
model = Model(inputs=sequence_input, outputs=output)

model.summary()

from tensorflow.keras.utils  import plot_model
plot_model(model, show_shapes=True)

model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),
              optimizer=tf.keras.optimizers.Adam(1e-4),
              metrics=['accuracy'])

model.fit(train_ds, epochs=3, validation_data=test_ds)

